# Robustness-and-Vulnerability-of-Safeguarding-in-Large-Language-Models

** This project is under active development ** 
This repository will hold the code for safeguarding experiments against large language models using Microsoft's PromptBench framework. 

Four chosen models will be tested against four datasets of harmful prompts within prompt leaking and prompt injection categories. Models will be tested for robustness against prompt attacks and vulnerabilities will be analysed and highlighted for future research. 


